{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation_with_Recurrent_NN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JairParra/ANOVA_with_R/blob/master/Text_generation_with_Recurrent_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZTFSXjHiFd1",
        "colab_type": "text"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jul 17 23:32:57 2019\n",
        "\n",
        "@author: jairp\n",
        "\"\"\"\n",
        "\n",
        "### *** Text Generation with a Recurrent NN *** ### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqHoFz28iIQm",
        "colab_type": "code",
        "outputId": "f89e6031-c406-4e9d-a939-adef11bf59b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### *** Text Generation with a Recurrent NN *** ### \n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "eager = tf.executing_eagerly() \n",
        "print(eager)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXycR604ilul",
        "colab_type": "code",
        "outputId": "04b51645-7ff6-4b96-f780-7789ff012925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Download the Shakespear dataset\n",
        "\n",
        "# https://raw.githubusercontent.com/mxw/grmr/master/src/finaltests/bible.txt\n",
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# path_to_file = tf.keras.utils.get_file('bible.txt', 'https://raw.githubusercontent.com/mxw/grmr/master/src/finaltests/bible.txt')\n",
        "path_to_file = tf.keras.utils.get_file('trump.txt', 'https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt')\n",
        "# path_to_file = tf.keras.utils.get_file('pishing.txt', 'https://raw.githubusercontent.com/JairParra/FakePisher/master/data_raw/fradulent_emails.txt?token=AF7ALARG2LDBSEGITAV4YWC5HIT52')\n",
        "\n",
        "\n",
        "\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f5Qyv5aoPiz",
        "colab_type": "code",
        "outputId": "b6b42b6c-2330-4b0a-afa2-48db648fc317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "## Read the Data\n",
        "\n",
        "\n",
        "# Read, then decode for py2 compat\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "# length of text is the number of characters in it \n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# Take a look at the first 250 cahracters in text\n",
        "print(text[:500])\n",
        "\n",
        "# The unique characters in the file \n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 903947 characters\n",
            "﻿SPEECH 1\r\n",
            "\r\n",
            "\r\n",
            "...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair press; he doesn't get it.  It's just not fair.  And I have to tell you I'm here, and very strongly here, because I have great respect for Steve King and have great respect likewise for Citizens United, David and everybody, and tremendous resect for the Tea Party.  Also, also the people of Iowa.  They have something in common.  Hard-working people.  They want to work, they want to make the country \n",
            "94 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsVh95ikoUsW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOdKDrRzoXmM",
        "colab_type": "code",
        "outputId": "e83e1953-ab76-4772-97fe-183275fbe1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "### Process the text ### \n",
        "\n",
        "## Vectorize the text  \n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "\n",
        "# convert the text to int array \n",
        "text_as_int = np.array([char2idx[c] for c in text] )\n",
        "\n",
        "# NOw we have an integer representation for each characters. Notice that \n",
        "# we mapped the character as indexes from 0 to len(unique) \n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(40)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "    \n",
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(\n",
        "    repr(text[:13]), text_as_int[:13]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '$' :   5,\n",
            "  '%' :   6,\n",
            "  '&' :   7,\n",
            "  \"'\" :   8,\n",
            "  '(' :   9,\n",
            "  ')' :  10,\n",
            "  ',' :  11,\n",
            "  '-' :  12,\n",
            "  '.' :  13,\n",
            "  '/' :  14,\n",
            "  '0' :  15,\n",
            "  '1' :  16,\n",
            "  '2' :  17,\n",
            "  '3' :  18,\n",
            "  '4' :  19,\n",
            "  '5' :  20,\n",
            "  '6' :  21,\n",
            "  '7' :  22,\n",
            "  '8' :  23,\n",
            "  '9' :  24,\n",
            "  ':' :  25,\n",
            "  ';' :  26,\n",
            "  '=' :  27,\n",
            "  '?' :  28,\n",
            "  '@' :  29,\n",
            "  'A' :  30,\n",
            "  'B' :  31,\n",
            "  'C' :  32,\n",
            "  'D' :  33,\n",
            "  'E' :  34,\n",
            "  'F' :  35,\n",
            "  'G' :  36,\n",
            "  'H' :  37,\n",
            "  'I' :  38,\n",
            "  'J' :  39,\n",
            "  ...\n",
            "}\n",
            "'\\ufeffSPEECH 1\\r\\n\\r\\n' ---- characters mapped to int ---- > [93 48 45 34 34 32 37  2 16  1  0  1  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n0MxrANof1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The prediction task \n",
        "\n",
        "# Given a character, or a sequence of cahracters, what is the most \n",
        "# probable next character? This is a the task we are training the model to perform. \n",
        "# The input to the model will be a sequence of characters, and we train the model \n",
        "# to predict the output. \n",
        "\n",
        "# Since RNNs maintain an internal state that depends on the previously \n",
        "# seen elements, given all the characters computed unil this moment, \n",
        "# what is the next characters? \n",
        "\n",
        "\n",
        "\n",
        "# ***************************************************************\n",
        "\n",
        "\n",
        "### Create trianing examples and targets ### \n",
        "\n",
        "\n",
        "# Next divide the text into example sequences. \n",
        "# Each input sequence will contain seq_length characters from the text. \n",
        "# For each input seq, the corresponding targets contain the same length \n",
        "#   of except shifted on echaracter to the right. \n",
        "# So break the text into chunks of seq_length+1. For example, \n",
        "#   say se_length is 4 and out text is \"Hello\". The input seuaence \n",
        "#   will be \"Hell\" and the target \"ello\". \n",
        "\n",
        "# To do this, first use the  tf.data.Dataset.from_tensor_slices function to \n",
        "#   convert the text vector into a stream of character indices.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF12I6qiohf_",
        "colab_type": "code",
        "outputId": "334a5b06-fbc9-427b-b334-ff1471883ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(20):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "S\n",
            "P\n",
            "E\n",
            "E\n",
            "C\n",
            "H\n",
            " \n",
            "1\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            ".\n",
            ".\n",
            ".\n",
            "T\n",
            "h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dow_9ZBipcL3",
        "colab_type": "code",
        "outputId": "c5f654bf-1564-449b-d5c6-43eb7351bbc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The *batch* method lets us easily conver these individuals characters \n",
        "#    to sequences of the desired size. \n",
        "    \n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\\ufeffSPEECH 1\\r\\n\\r\\n\\r\\n...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair p\"\n",
            "\"ress; he doesn't get it.  It's just not fair.  And I have to tell you I'm here, and very strongly her\"\n",
            "'e, because I have great respect for Steve King and have great respect likewise for Citizens United, D'\n",
            "'avid and everybody, and tremendous resect for the Tea Party.  Also, also the people of Iowa.  They ha'\n",
            "'ve something in common.  Hard-working people.  They want to work, they want to make the country great'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-uXXmr6pl2v",
        "colab_type": "code",
        "outputId": "95cc0cbf-4d8d-4d95-a0ce-f77b53d1f1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# For each sequence, duplicate and shift it to form the \n",
        "# input and target text by using the map method to apply \n",
        "#  a simple function to each batch: \n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print the first examples and target values \n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"\\ufeffSPEECH 1\\r\\n\\r\\n\\r\\n...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair \"\n",
            "Target data: \"SPEECH 1\\r\\n\\r\\n\\r\\n...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair p\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN0KfZDwpyf9",
        "colab_type": "code",
        "outputId": "ef8fd629-8c87-419e-b1ab-ee818d745ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Each of the index of thes evectors are preocessed as one time step. \n",
        "# For the input at time step 0, the model received the index for \n",
        "# \"F\" and tris to predict the index for \"i\" as the next character. \n",
        "# At the next timestep, it does the same thing, but the RNN \n",
        "# considers the previous step context in addition to the \n",
        "# current input character. \n",
        "  \n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 93 ('\\ufeff')\n",
            "  expected output: 48 ('S')\n",
            "Step    1\n",
            "  input: 48 ('S')\n",
            "  expected output: 45 ('P')\n",
            "Step    2\n",
            "  input: 45 ('P')\n",
            "  expected output: 34 ('E')\n",
            "Step    3\n",
            "  input: 34 ('E')\n",
            "  expected output: 34 ('E')\n",
            "Step    4\n",
            "  input: 34 ('E')\n",
            "  expected output: 32 ('C')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Cmr01sp1kb",
        "colab_type": "code",
        "outputId": "f9ed78e7-8a54-423b-ba16-34124b68d3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ****************************************************************8\n",
        "    \n",
        "### Create training batches ### \n",
        "    \n",
        "# We used *tf.data* to split the text into manageable sequences. \n",
        "# But before fedding this data into the model, we need to tshuffle \n",
        "# the data and pack it into batches. \n",
        "    \n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXN6cZHip4uC",
        "colab_type": "code",
        "outputId": "abf7c34b-3f0f-4596-8151-31f9fed63473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# *******************************************************************\n",
        "\n",
        "### Build The Model ### \n",
        "\n",
        "# use tf.keras.Sequential to define the model. For this simple example, three \n",
        "# layers are used: \n",
        "\n",
        "    # tf.keras.layers.Embedding: \n",
        "        # The input layer. A trainable lookup table that will\n",
        "        # map the numbers of each character vector with \n",
        "        # *embedding_dim* dimensions. \n",
        "    # td.keras.layers.GRU: \n",
        "        # A type of RNN with size units=rnn_units \n",
        "        # (can also use a LSTM layer here) \n",
        "    # tf.keras.layers.Dense: \n",
        "        # The ouput layer, with *vocab_size* outputs. \n",
        "        \n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "# Next define a function to build the model \n",
        "# Use CuDNNGRU if runnin gon GPU \n",
        "\n",
        "print(\"Is GPU available: \", tf.test.is_gpu_available())\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "  lstm = tf.compat.v1.keras.layers.CuDNNLSTM\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  lstm =functools.partial(\n",
        "      tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
        "    \n",
        "    \n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "     rnn(rnn_units,\n",
        "         return_sequences=True,\n",
        "         recurrent_initializer='glorot_uniform', # first RNN layer\n",
        "         stateful=True),\n",
        "    lstm(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform', # first RNN layer\n",
        "        stateful=True),\n",
        "     \n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "# FOr each character, the model looks up the embedding runs the GRU \n",
        "# one timestep with the embedding as input, adn applies the dense layer \n",
        "# to generate logits predicting the log-likelihood of the next char\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is GPU available:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dstz6CGFqK09",
        "colab_type": "code",
        "outputId": "f6862a05-2ef5-463c-870d-f5fe95415b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# ************************************************************************\n",
        "\n",
        "### Try the model ### \n",
        "\n",
        "# Now run the model to see that it behaves as expected \n",
        "\n",
        "# First check the shape of the output: \n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "  \n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 94) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           24064     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm (CuDNNLSTM)       (64, None, 1024)          8396800   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 94)            96350     \n",
            "=================================================================\n",
            "Total params: 12,455,518\n",
            "Trainable params: 12,455,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfgqjeS6akMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3IBfEPNrAwx",
        "colab_type": "code",
        "outputId": "93553247-7590-4d05-afd2-0bc461985ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# To get actual predictions from the model we need to saomple \n",
        "# from the output ditribution, to get actual character indices. \n",
        "# This distribution is defined by the logits over the vocabulary. \n",
        "\n",
        "# NOTE: It's important to sample from this distribution as \n",
        "# taking the argmax of the distribution can easily get the model \n",
        "# stuck in a loop. \n",
        "\n",
        "\n",
        "# Try it for the first example in the batch: \n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "\n",
        "# This gives us, at each timestep, a prediction of the next character \n",
        "# index: \n",
        "\n",
        "print(sampled_indices) \n",
        "\n",
        "# Decode these to see the text predicted by this untrained model: \n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[76 87 40 22  8 58 34 73 26 43 66 71 87 69  0 30 91 43 35 81 63 80 86 82\n",
            " 37 79 59 91  2 43 22 74 54 49 87 42 85 35 92 52 31 34 25 12 78 11  5  4\n",
            " 72 90 62 79 32 61 33 18 17 20 72 20 47 20 15 17 47 66 63 23 55 23 76 74\n",
            " 18 28  4 73 10 86 45 36 57 90 73 22 66 17 15 64 13 29 33 61 25 34 66 81\n",
            " 45 17 78 10]\n",
            "Input: \n",
            " 'getting 21 people. And they have the right to do whatever they want from an enterprise standpoint. R'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'r—K7\\'_Eo;Nhm—k\\nA”NFwev–xHua” N7pYT—MéF…WBE:-t,$\"n“duCcD325n5R502Rhe8Z8rp3?\"o)–PG]“o7h20f.@Dc:EhwP2t)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yZQ47blrHLA",
        "colab_type": "code",
        "outputId": "9cd97ee7-acab-4c72-98ce-512cf6f6900c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# *****************************************************************\n",
        "\n",
        "### Train the model ### \n",
        "\n",
        "# At this point the problem can be treated as a standard classification \n",
        "# problem. Given the previous RNN state, and the input at this time step, \n",
        "# predict the class of the next character. \n",
        "\n",
        "\n",
        "## Attach an optimizer, and a loss function \n",
        "\n",
        "# The standard *tf.keras.losses.sparse_softmax_crossentropy* loss \n",
        "# function works in this case because it is applied across the last \n",
        "# dimension of the predictions. \n",
        "\n",
        "# Because our model returns logits, we need to set the from_logits flag. \n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "# COnfigure the training procedure using the tf.leras.Model.compile method. \n",
        "# We'll use *tf.train.AdamOptimizer* with defautl arguments and \n",
        "# the loss funciton. \n",
        "\n",
        "model.compile(\n",
        "        optimizer=tf.compat.v1.train.AdamOptimizer(), \n",
        "        loss = loss)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 94)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.5427647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqy3WPVFrRME",
        "colab_type": "code",
        "outputId": "b28481a0-9225-43ef-83a9-f723fe79a803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "# ****************************************************************\n",
        "\n",
        "### Configure checkpoints ### \n",
        "\n",
        "# Use a *tf.keras.callbacks.ModelCheckpoint* to ensure that checkpoints are \n",
        "# saved durinf training: \n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "\n",
        "# set up gpu \n",
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "\n",
        "\n",
        "# **************************************************************\n",
        "\n",
        "### Execute the training ### \n",
        "\n",
        "# To keep training time reasonable, use 3 epochs to train the model. \n",
        "# In Colab, set the runtime to GPU for faster training. \n",
        "\n",
        "EPOCHS=12\n",
        "\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, \n",
        "                    steps_per_epoch=steps_per_epoch, \n",
        "                    callbacks=[checkpoint_callback])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "141/141 [==============================] - 48s 343ms/step - loss: 2.5557\n",
            "Epoch 2/12\n",
            "141/141 [==============================] - 46s 323ms/step - loss: 1.5492\n",
            "Epoch 3/12\n",
            "141/141 [==============================] - 46s 323ms/step - loss: 1.2701\n",
            "Epoch 4/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 1.1560\n",
            "Epoch 5/12\n",
            "141/141 [==============================] - 46s 323ms/step - loss: 1.0841\n",
            "Epoch 6/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 1.0302\n",
            "Epoch 7/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.9834\n",
            "Epoch 8/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.9385\n",
            "Epoch 9/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.8998\n",
            "Epoch 10/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.8584\n",
            "Epoch 11/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.8223\n",
            "Epoch 12/12\n",
            "141/141 [==============================] - 46s 324ms/step - loss: 0.7841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj9cN8K5x-IZ",
        "colab_type": "code",
        "outputId": "ca56e9e5-6334-492e-87c5-4ac33c40f883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# ****************************************************************8\n",
        "\n",
        "### Generate text ### \n",
        "\n",
        "## Restore the latest checkpoint \n",
        "\n",
        "# To keep this prediction simple, use a batch size of 1. \n",
        "\n",
        "# Because of the way the RNN state is passed form timestep to \n",
        "# timestep, the model only accepts a fixed batch size once built. \n",
        "\n",
        "# To run the model with a different batch_size , we need to rebuild \n",
        "# the model and restore the weights from the checkpoint\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            24064     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (1, None, 1024)           8396800   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 94)             96350     \n",
            "=================================================================\n",
            "Total params: 12,455,518\n",
            "Trainable params: 12,455,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk0Ehm5IyCF2",
        "colab_type": "code",
        "outputId": "a5a29734-b974-46bc-d2b7-750f61bb230b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        " \n",
        "# **************************************************************\n",
        "\n",
        "### The prediction loop ### \n",
        "\n",
        "# The following code block generates the text: \n",
        "\n",
        "    # - It starts y choosing a start string, initializing the \n",
        "    #   RNN state and setting the number of characters to generate. \n",
        "    # - Get the prediction distribution of the next character using \n",
        "    #   the start string and the RNN state, \n",
        "    # - Use a multinomial distribution to calculate the index \n",
        "    #   of the predicted character. Use this predicted character \n",
        "    #   as out next input to the model. \n",
        "    #   The RNN state returned by the model is fed back into the\n",
        "    #   model so that it now has more contexxt, instead of only\n",
        "    #   one word. After predicting the next word, the modified \n",
        "    #   RNN states are again fed back into the model, which is \n",
        "    #   how it learns as it gets more context from the \n",
        "    #   previously predicted words. \n",
        "    \n",
        "# Looking at the generat text, you'll see that the model knows \n",
        "# when to capitalize and make paragraphs,  and it imitates a \n",
        "# Shakespear -like writing vocabulary. With the mal number of \n",
        "# training epochs, it has not yet learned to form coherent sentences. \n",
        "\n",
        "\n",
        "def generate_text(model, start_string, num_generate = 1000,\n",
        "                 temperature = 1.0):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = num_generate\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temperature\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "\n",
        "print(generate_text(model, start_string=u\"Thank you\\n\", num_generate=2000, \n",
        "                    temperature=0.7))\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 21:18:11.101668 139847443584896 deprecation.py:323] From <ipython-input-16-c5050f58434a>:31: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Thank you\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "...Well, he just responded to put America back there for loving the election.\r\n",
            "So let me just tell you that. You’ve never spent powerful crime, and I said, \"What are the thing with Hispanics who I’ve said I don’t have to say that we’re going to spend like some monster. You know, we have people that are run by people who rigged the system — we have to build a wall.\r\n",
            "But you know what? I don’t want to do business with the evangelical support.\r\n",
            "So you know they had this good thing.\r\n",
            "So we have to go up to my office, and we can do very well. Some of them when them jobs. Okay? So they do is the president who wants to go and senators – and I don’t know what the hell does not sir. It’s the first time I can build the wall. One of these people are using people that work like in the world. I mean, this and I had a big number of reasons. I would have been taken away from us. I’m going to do that. Okay? I want to do a good job. It’s the way it is where right on television with with evangelical Medicare. We’re going to bring back our jobs back from Mexico. They don’t want to bring up. And it was with the people – a lot of time he spokes that we need to make America great again. So they do that when I raise it or not, moving to $10 million. I think he’d be saying we have all of the people from China – our inner coming in over a lot of money.\r\n",
            "We’ve got a lot of good jobs. We’re going to hurt.\r\n",
            "\r\n",
            "\r\n",
            "The debate was the worst legation of the United States again. We’re going to build a wall. We have a lot of money to be a respect for him and you can always do that and you know what? If you’re a journey goal more important issues at all of it.  I'm going to go after my own moment of time whenever to be honest.\r\n",
            "But these are people that are opportunity as the reporter, who endorsed me, I can tell you.\r\n",
            "So they were talking about it.  Benghazi.  They are like a very big inversion. Right? Right? It’s not 100%. Some week planes all to cut the way. I have been no successfou know it, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCGLRxO9vsyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Advanced: Customized Training ### \n",
        "\n",
        "# The above trainign rpocedure is simkple. ut does not give you much control. \n",
        "\n",
        "# So now that you've seen how to run the model manually let'sunpack the training\n",
        "# loop, and implement it ourselves. This gives a starting poont, for example,  \n",
        "# to implement curriculum learning to help stabilize the model's open-loop output. \n",
        "\n",
        "# We will use tf.GradientTape to track the gradients. \n",
        "\n",
        "# This procedure works as follows: \n",
        "\n",
        "  # - First, initialize the RNN state. We do this by calling the\n",
        "  #   tf.keras.Model.reset_sates method. \n",
        "  # - Next, iterate over the dataset(batch by batch), and calculate the \n",
        "  #   predictions associated with each. \n",
        "  # - Open a tf.GradientTape, and calculate the predictions and loss \n",
        "  #   in that context. \n",
        "  # - Calculate the gradients of the loss with respect to the model vairables\n",
        "  #   using the tf.GradientTape.grads method. \n",
        "  # - Finally, take a step downwards by using the optimizer's \n",
        "  #   tf.train.Optimizer.apply_gradients method. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPKQ0MmiUGbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "vocab_size= len(vocab), \n",
        "embedding_dim=embedding_dim, \n",
        "rnn_units=rnn_units,\n",
        "batch_size=BATCH_SIZE) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3tZb2YeUTdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# # Training step \n",
        "# EPOCHS = 7\n",
        "\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     start = time.time()\n",
        "\n",
        "#     # initializing the hidden state at the start of every epoch\n",
        "#     # initially hidden is None\n",
        "#     hidden = model.reset_states()\n",
        "\n",
        "#     for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "#           with tf.GradientTape() as tape:\n",
        "#               # feeding the hidden state back into the model\n",
        "#               # This is the interesting step\n",
        "#               predictions = model(inp)\n",
        "#               loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "\n",
        "#           grads = tape.gradient(loss, model.trainable_variables)\n",
        "#           optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "#           if batch_n % 100 == 0:\n",
        "#               template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "#               print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "#     # saving (checkpoint) the model every 5 epochs\n",
        "#     if (epoch + 1) % 5 == 0:\n",
        "#       model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "#     print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "#     print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "# model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7JmfAZRkDbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Try the model again bu this time checking the gradient \n",
        "\n",
        "# print(generate_text(model, start_string=u\"Thank you\", num_generate=2000, \n",
        "#                     temp=1.0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}